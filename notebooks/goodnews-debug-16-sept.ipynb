{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T16:14:05.535661Z",
     "start_time": "2020-09-16T16:14:05.511722Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import dateutil.parser\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import praw\n",
    "\n",
    "import azure.functions as func\n",
    "\n",
    "\n",
    "def mask_df(in_df, col='', filt=''):\n",
    "    \"\"\" Filters dataframe using user defined column and filter\"\"\"\n",
    "    mask = in_df[col] == filt\n",
    "    return in_df[mask]\n",
    "\n",
    "\n",
    "def get_all_article_urls(url, website_attributes):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    base_url = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "    print('base url is', base_url)\n",
    "\n",
    "    if base_url in website_attributes:\n",
    "        print('web parameters found, parsing')\n",
    "        params = website_attributes[base_url]\n",
    "\n",
    "        all_articles = soup.find(params['all_article_tag'],\n",
    "                                 attrs={'class': params['all_article_class']})\n",
    "        all_article_urls = all_articles.find_all(params['article_tag'])\n",
    "        return_urls = []\n",
    "        for article_url in all_article_urls:\n",
    "            print('base url is', base_url)\n",
    "            if 'bbc' in base_url:\n",
    "                try:\n",
    "                    return_urls.append(base_url+article_url.find('a')['href'])\n",
    "                except TypeError:\n",
    "                    \"error grabbing article url - maybe the article type is nonstandard\"\n",
    "            else:\n",
    "                return_urls.append(article_url.find('a')['href'])\n",
    "        print('number of article URLs found:', len(return_urls))\n",
    "    return return_urls\n",
    "\n",
    "\n",
    "def get_article_details(url, website_attributes):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    base_url = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "#     print('base url is', base_url)\n",
    "\n",
    "    article_title = soup.title.text.strip()\n",
    "\n",
    "    if base_url in website_attributes:\n",
    "        # print('web parameters found, parsing')\n",
    "        params = website_attributes[base_url]\n",
    "\n",
    "        try:\n",
    "            article_date = soup.find(params['date_tag']\n",
    "                                 , attrs={'class': params['date_class']})\n",
    "            article_date = article_date.text.strip()\n",
    "        except AttributeError:\n",
    "            print('error - article date could not be found')\n",
    "            return None, None, None, None\n",
    "\n",
    "        try:\n",
    "            postContent = soup.find(params['content_tag']\n",
    "                                    , attrs={'class': params['content_class']})\n",
    "        except AttributeError:\n",
    "            print('error - article content could not be found')\n",
    "            return None, None, None, None\n",
    "\n",
    "        article_text = []\n",
    "        for para in postContent.find_all('p'):\n",
    "            article_text.append(para.text.strip())\n",
    "\n",
    "        return article_title, article_date, article_text, base_url\n",
    "    else:\n",
    "        print('url not found in website parameters. returning None types')\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def reddit_auth(auth_json):\n",
    "\n",
    "    reddit = praw.Reddit(username=auth_json['username'],\n",
    "                         password=auth_json['password'],\n",
    "                         client_id=auth_json['client_id'],\n",
    "                         client_secret=auth_json['client_secret'],\n",
    "                         user_agent=auth_json['user_agent'])\n",
    "    return reddit\n",
    "\n",
    "\n",
    "def get_top_reddit_posts(reddit, subreddit, top, lim):\n",
    "    posts = reddit.subreddit(subreddit).top(top)\n",
    "    df = pd.DataFrame()\n",
    "    cols = ['url', 'title', 'date', 'text', 'root_url']\n",
    "\n",
    "    for post in posts:\n",
    "        utc = post.created_utc\n",
    "        fmt = '%d %B, %Y'\n",
    "        date = datetime.datetime.utcfromtimestamp(utc).strftime(fmt)\n",
    "        root_url = re.match('^.+?[^\\/:](?=[?\\/]|$)', post.url).group(0)\n",
    "\n",
    "        data = [post.url, post.title, date, '', root_url]\n",
    "\n",
    "        df = df.append(pd.DataFrame([data], columns=cols))\n",
    "\n",
    "    return df.head(lim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T16:15:38.101557Z",
     "start_time": "2020-09-16T16:15:28.867527Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base url is https://www.bbc.co.uk\n",
      "web parameters found, parsing\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "base url is https://www.bbc.co.uk\n",
      "number of article URLs found: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▌                                                                         | 1/8 [00:00<00:02,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - article date could not be found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|█████████████████████                                                               | 2/8 [00:00<00:02,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - article date could not be found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▌                                                    | 3/8 [00:01<00:02,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - article date could not be found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 4/8 [00:01<00:01,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - article date could not be found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|████████████████████████████████████████████████████▌                               | 5/8 [00:02<00:01,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - article date could not be found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|█████████████████████████████████████████████████████████████████████████▌          | 7/8 [00:04<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - article date could not be found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:05<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error - article date could not be found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rhyl lifeboat crew rescue seagull-chasing dog from sea - BBC News already in podio... ignoring\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def main(mytimer: func.TimerRequest) -> None:\n",
    "#     utc_timestamp = datetime.datetime.utcnow().replace(\n",
    "#         tzinfo=datetime.timezone.utc).isoformat()\n",
    "\n",
    "    # we note down the HTML class types and names that contain the news. \n",
    "    # This works by first looking at a page containing many news stories,\n",
    "    # such as today's news. Then it looks for URLs using all_article_{var}\n",
    "    # key:value pairs. Then it navigates to each article, pulls out the \n",
    "    # title, date and core text content (although the text is unused as of \n",
    "    # 27/08/2020 due to copyright concerns)\n",
    "    \n",
    "website_attributes = {\n",
    "    'https://www.bbc.co.uk': {\n",
    "        'all_article_tag': 'div',\n",
    "        'all_article_class': 'gel-layout gel-layout--center',\n",
    "        'article_tag': 'article',\n",
    "        'date_tag': 'div',\n",
    "        'date_class': 'date date--v2',\n",
    "        'content_tag': 'div',\n",
    "        'content_class': 'story-body__inner'\n",
    "    }\n",
    "}\n",
    "bbc_url = 'https://www.bbc.co.uk/news/topics/cx2pk70323et/uplifting-stories'\n",
    "\n",
    "df_scraped = pd.DataFrame()\n",
    "cols = ['url', 'title', 'date', 'text', 'root_url']\n",
    "\n",
    "todays_bbc_articles = get_all_article_urls(bbc_url, website_attributes)\n",
    "\n",
    "for url in tqdm(todays_bbc_articles):\n",
    "    article_title, article_date, article_text, base_url = get_article_details(url, website_attributes)\n",
    "    df_scraped = df_scraped.append(pd.DataFrame([[url, article_title, article_date, \n",
    "                                                  article_text, base_url]], \n",
    "                                        columns=cols))\n",
    "\n",
    "\n",
    "# add in reddit /r/upliftingnews to dataframe\n",
    "reddit_data = {\"client_id\": \"9sIhMkT4rrMQjA\",\n",
    "    \"client_secret\": \"LzQR8Qkql1FueFxrQ-5wxK5Fq9E\",\n",
    "    \"user_agent\": \"dona_lic_app\",\n",
    "    \"username\": \"dona_lic\",\n",
    "    \"password\": \"Kgvv9LTy%%8@5WlY\"}\n",
    "\n",
    "reddit = reddit_auth(reddit_data)\n",
    "reddit_df = get_top_reddit_posts(reddit, 'upliftingnews', 'day', 3)\n",
    "df_scraped = df_scraped.append(reddit_df)\n",
    "\n",
    "df_scraped = df_scraped.dropna(how='any')\n",
    "df_scraped['date_parsed'] = [dateutil.parser.parse(x).date() for x in df_scraped['date']]\n",
    "df_scraped['provider_parsed'] = [x.split('.')[1] for x in df_scraped['root_url']]\n",
    "\n",
    "# Now we have all the data we read in the titles from Podio to\n",
    "# check if any articles already exist. If not, they are posted to Podio.\n",
    "\n",
    "# using james' shim layer to define api URLs\n",
    "base_url = 'https://goodnewsmicroapp.azurewebsites.net/api'\n",
    "auth_url = f'{base_url}/PodioAuth'\n",
    "\n",
    "# store podio auth info locally and get auth token here \n",
    "podio_data = {\n",
    "    \"app_id\": \"25058801\",\n",
    "    \"app_token\": \"683029008df9495a8947c90a38f75ce9\",\n",
    "    \"client_id\": \"goodnews\",\n",
    "    \"client_secret\": \"wrCUCZSxFuPmPZpm7f9iRWm9J4mS6VshbDuXxjNYAHL5RAMTKOFy4VSwHZ4w3csk\",\n",
    "    \"grant_type\": \"client_credentials\"\n",
    "}\n",
    "app_id = podio_data['app_id']\n",
    "podio_resp = requests.post(auth_url, data=podio_data)\n",
    "\n",
    "if not podio_resp.ok:\n",
    "    raise Exception(\"Auth failed\", podio_resp)\n",
    "\n",
    "podio_resp = podio_resp.json()\n",
    "token = podio_resp['access_token']\n",
    "\n",
    "headers = {\n",
    "\"content-type\": \"application/json\",\n",
    "\"authorization\": \"Bearer \" + token,\n",
    "\"x-podio-client-id\": 'x',\n",
    "\"x-podio-client-secret\": 'x'\n",
    "}\n",
    "\n",
    "# now we have auth token in a header we can return item data\n",
    "item_url = f'{base_url}/PodioProxy/item/app/{app_id}/filter/'\n",
    "data = {\n",
    "    \"limit\":500,\n",
    "    \"offset\":0,\n",
    "    \"filters\":{\n",
    "        }\n",
    "    }\n",
    "\n",
    "all_data = requests.post(item_url, headers=headers, json=data).json()\n",
    "df_from_podio = pd.json_normalize(all_data)\n",
    "df_from_podio = df_from_podio.dropna()\n",
    "\n",
    "# we treat the article titles as UIDs and check against them\n",
    "# to see if they exist, and if not post to Podio\n",
    "try:\n",
    "    all_titles = df_from_podio['Title'].values\n",
    "except KeyError:\n",
    "    print('Error with the return JSON from Podio.\\\n",
    "        Have you sent in the correct format, or are you rate limited?')\n",
    "\n",
    "# since James' shim doesn't work for posting, we redefine our request URL\n",
    "# and also remove the two proprietary header entries\n",
    "post_headers = {\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": \"Bearer \" + token,\n",
    "    }\n",
    "post_item_url = f\"https://api.podio.com/item/app/{app_id}/\"\n",
    "\n",
    "for index, row in df_scraped.iterrows():  \n",
    "    if row.title.strip() in all_titles:\n",
    "        print(f'{row.title} already in podio... ignoring')\n",
    "        pass\n",
    "    else:    \n",
    "        item = {\n",
    "            \"title\": str(row.title),\n",
    "            \"url-3\": str(row.url),\n",
    "            \"date-3\": str(row.date_parsed),\n",
    "            \"provider\": str(row.provider_parsed),\n",
    "            \"upvotes\": 0,\n",
    "            \"downvotes\": 0,\n",
    "        }\n",
    "        data = {\n",
    "            \"fields\": item\n",
    "        }\n",
    "\n",
    "        item_resp = requests.post(post_item_url, json=data, headers=post_headers)\n",
    "        if not item_resp.ok:\n",
    "            raise Exception(\"Post failed\", item_resp)\n",
    "\n",
    "#     if mytimer.past_due:\n",
    "#         logging.info('The timer is past due!')\n",
    "\n",
    "#     logging.info('Python timer trigger function ran at %s', utc_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T16:15:38.128347Z",
     "start_time": "2020-09-16T16:15:38.103553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>root_url</th>\n",
       "      <th>date_parsed</th>\n",
       "      <th>provider_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.bbc.co.uk/news/uk-wales-53974775?i...</td>\n",
       "      <td>Rhyl lifeboat crew rescue seagull-chasing dog ...</td>\n",
       "      <td>31 August 2020</td>\n",
       "      <td>[A lifeboat crew had to rescue a dog after it ...</td>\n",
       "      <td>https://www.bbc.co.uk</td>\n",
       "      <td>2020-08-31</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://kval.com/news/local/national-guard-loa...</td>\n",
       "      <td>National Guard loaded helicopters 'to the abso...</td>\n",
       "      <td>15 September, 2020</td>\n",
       "      <td></td>\n",
       "      <td>https://kval.com</td>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.latimes.com/california/story/2020-...</td>\n",
       "      <td>L.A. County coronavirus numbers fall back to p...</td>\n",
       "      <td>15 September, 2020</td>\n",
       "      <td></td>\n",
       "      <td>https://www.latimes.com</td>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>latimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.msn.com/en-us/news/good-news/911-d...</td>\n",
       "      <td>911 Dispatcher Saves Lives of Baby and 71-Year...</td>\n",
       "      <td>15 September, 2020</td>\n",
       "      <td></td>\n",
       "      <td>https://www.msn.com</td>\n",
       "      <td>2020-09-15</td>\n",
       "      <td>msn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.bbc.co.uk/news/uk-wales-53974775?i...   \n",
       "0  https://kval.com/news/local/national-guard-loa...   \n",
       "0  https://www.latimes.com/california/story/2020-...   \n",
       "0  https://www.msn.com/en-us/news/good-news/911-d...   \n",
       "\n",
       "                                               title                date  \\\n",
       "0  Rhyl lifeboat crew rescue seagull-chasing dog ...      31 August 2020   \n",
       "0  National Guard loaded helicopters 'to the abso...  15 September, 2020   \n",
       "0  L.A. County coronavirus numbers fall back to p...  15 September, 2020   \n",
       "0  911 Dispatcher Saves Lives of Baby and 71-Year...  15 September, 2020   \n",
       "\n",
       "                                                text                 root_url  \\\n",
       "0  [A lifeboat crew had to rescue a dog after it ...    https://www.bbc.co.uk   \n",
       "0                                                            https://kval.com   \n",
       "0                                                     https://www.latimes.com   \n",
       "0                                                         https://www.msn.com   \n",
       "\n",
       "  date_parsed provider_parsed  \n",
       "0  2020-08-31             bbc  \n",
       "0  2020-09-15             com  \n",
       "0  2020-09-15         latimes  \n",
       "0  2020-09-15             msn  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scraped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
