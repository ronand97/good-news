{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T14:19:51.946916Z",
     "start_time": "2020-10-05T14:19:50.718781Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import dateutil.parser\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import praw\n",
    "\n",
    "import azure.functions as func\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T14:24:38.703494Z",
     "start_time": "2020-10-05T14:24:38.683543Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def mask_df(in_df, col='', filt=''):\n",
    "    \"\"\" Filters dataframe using user defined column and filter\"\"\"\n",
    "    mask = in_df[col] == filt\n",
    "    return in_df[mask]\n",
    "\n",
    "\n",
    "def get_all_article_urls(url, website_attributes):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    base_url = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "    print('base url is', base_url)\n",
    "\n",
    "    if base_url in website_attributes:\n",
    "        print('web parameters found, parsing')\n",
    "        params = website_attributes[base_url]\n",
    "\n",
    "        all_articles = soup.find(params['all_article_tag'],\n",
    "                                 attrs={'class': params['all_article_class']})\n",
    "        all_article_urls = all_articles.find_all(params['article_tag'])\n",
    "        return_urls = []\n",
    "        for article_url in all_article_urls:\n",
    "            if 'bbc' in base_url:\n",
    "                try:\n",
    "                    return_urls.append(base_url+article_url.find('a')['href'])\n",
    "                except TypeError:\n",
    "                    \"error grabbing article url - maybe the article type is nonstandard\"\n",
    "            else:\n",
    "                return_urls.append(article_url.find('a')['href'])\n",
    "            time.sleep(1) # limit num requests in short period of time\n",
    "        print('number of article URLs found:', len(return_urls))\n",
    "    return return_urls\n",
    "\n",
    "\n",
    "def get_article_details(url, website_attributes):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    base_url = re.match('^.+?[^\\/:](?=[?\\/]|$)', url).group(0)\n",
    "#     print('base url is', base_url)\n",
    "\n",
    "    article_title = soup.title.text.strip()\n",
    "\n",
    "    if base_url in website_attributes:\n",
    "        # print('web parameters found, parsing')\n",
    "        params = website_attributes[base_url]\n",
    "\n",
    "        try:\n",
    "            article_date = soup.find(params['date_tag']\n",
    "                                 , attrs={'class': params['date_class']})\n",
    "            article_date = article_date.text.strip()\n",
    "        except AttributeError:\n",
    "            print('error - article date could not be found')\n",
    "            return None, None, None, None\n",
    "\n",
    "        try:\n",
    "            postContent = soup.find(params['content_tag']\n",
    "                                    , attrs={'class': params['content_class']})\n",
    "        except AttributeError:\n",
    "            print('error - article content could not be found')\n",
    "            return None, None, None, None\n",
    "\n",
    "        article_text = []\n",
    "        for para in postContent.find_all('p'):\n",
    "            article_text.append(para.text.strip())\n",
    "\n",
    "        return article_title, article_date, article_text, base_url\n",
    "    else:\n",
    "        print('url not found in website parameters. returning None types')\n",
    "        return None, None, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T14:20:07.475567Z",
     "start_time": "2020-10-05T14:20:07.466589Z"
    }
   },
   "outputs": [],
   "source": [
    "def reddit_auth(auth_json):\n",
    "\n",
    "    reddit = praw.Reddit(username=auth_json['username'],\n",
    "                         password=auth_json['password'],\n",
    "                         client_id=auth_json['client_id'],\n",
    "                         client_secret=auth_json['client_secret'],\n",
    "                         user_agent=auth_json['user_agent'])\n",
    "    return reddit\n",
    "\n",
    "\n",
    "def get_top_reddit_posts(reddit, subreddit, top, lim):\n",
    "    posts = reddit.subreddit(subreddit).top(top)\n",
    "    df = pd.DataFrame()\n",
    "    cols = ['url', 'title', 'date', 'text', 'root_url']\n",
    "\n",
    "    for post in posts:\n",
    "        utc = post.created_utc\n",
    "        fmt = '%d %B, %Y'\n",
    "        date = datetime.datetime.utcfromtimestamp(utc).strftime(fmt)\n",
    "        root_url = re.match('^.+?[^\\/:](?=[?\\/]|$)', post.url).group(0)\n",
    "\n",
    "        data = [post.url, post.title, date, '', root_url]\n",
    "\n",
    "        df = df.append(pd.DataFrame([data], columns=cols))\n",
    "\n",
    "    return df.head(lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T14:26:16.682371Z",
     "start_time": "2020-10-05T14:26:15.362834Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.0.0 of praw is outdated. Version 7.1.0 was released Tuesday June 23, 2020.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we note down the HTML class types and names that contain the news. \n",
    "# This works by first looking at a page containing many news stories,\n",
    "# such as today's news. Then it looks for URLs using all_article_{var}\n",
    "# key:value pairs. Then it navigates to each article, pulls out the \n",
    "# title, date and core text content (although the text is unused as of \n",
    "# 27/08/2020 due to copyright concerns)\n",
    "website_attributes = {\n",
    "    'https://www.bbc.co.uk': {\n",
    "        'all_article_tag': 'div',\n",
    "        'all_article_class': 'gel-layout gel-layout--center',\n",
    "        'article_tag': 'article',\n",
    "        'date_tag': 'div',\n",
    "        'date_class': 'date date--v2',\n",
    "        'content_tag': 'div',\n",
    "        'content_class': 'story-body__inner'\n",
    "    }\n",
    "}\n",
    "bbc_url = 'https://www.bbc.co.uk/news/topics/cx2pk70323et/uplifting-stories'\n",
    "\n",
    "df_scraped = pd.DataFrame()\n",
    "cols = ['url', 'title', 'date', 'text', 'root_url']\n",
    "\n",
    "# todays_bbc_articles = get_all_article_urls(bbc_url, website_attributes)\n",
    "\n",
    "# for url in tqdm(todays_bbc_articles):\n",
    "#     article_title, article_date, article_text, base_url = get_article_details(url, website_attributes)\n",
    "#     df_scraped = df_scraped.append(pd.DataFrame([[url, article_title, article_date, \n",
    "#                                                   article_text, base_url]], \n",
    "#                                         columns=cols))\n",
    "\n",
    "# df_scraped = df_scraped.dropna(how='any')\n",
    "\n",
    "# df_scraped['date_parsed'] = [dateutil.parser.parse(x).date() for x in df_scraped['date']]\n",
    "# df_scraped['provider_parsed'] = [x.split('.')[1] for x in df_scraped['root_url']]\n",
    "\n",
    "# add in reddit /r/upliftingnews to dataframe\n",
    "reddit_data = {\"client_id\": \"9sIhMkT4rrMQjA\",\n",
    "    \"client_secret\": \"LzQR8Qkql1FueFxrQ-5wxK5Fq9E\",\n",
    "    \"user_agent\": \"dona_lic_app\",\n",
    "    \"username\": \"dona_lic\",\n",
    "    \"password\": \"Kgvv9LTy%%8@5WlY\"}\n",
    "\n",
    "reddit = reddit_auth(reddit_data)\n",
    "reddit_df = get_top_reddit_posts(reddit, 'upliftingnews', 'day', 3)\n",
    "df_scraped = df_scraped.append(reddit_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T14:26:18.989363Z",
     "start_time": "2020-10-05T14:26:18.979391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>root_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.autocar.co.uk/car-news/industry/an...</td>\n",
       "      <td>Volvo reports that EVs make up for their produ...</td>\n",
       "      <td>04 October, 2020</td>\n",
       "      <td></td>\n",
       "      <td>https://www.autocar.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.bbc.com/news/world-australia-54417343</td>\n",
       "      <td>Tasmanian devils have been reintroduced into t...</td>\n",
       "      <td>05 October, 2020</td>\n",
       "      <td></td>\n",
       "      <td>https://www.bbc.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.nytimes.com/2020/09/27/opinion/pbs...</td>\n",
       "      <td>Happy anniversary, PBS: today marks 50 years o...</td>\n",
       "      <td>04 October, 2020</td>\n",
       "      <td></td>\n",
       "      <td>https://www.nytimes.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.autocar.co.uk/car-news/industry/an...   \n",
       "0  https://www.bbc.com/news/world-australia-54417343   \n",
       "0  https://www.nytimes.com/2020/09/27/opinion/pbs...   \n",
       "\n",
       "                                               title              date text  \\\n",
       "0  Volvo reports that EVs make up for their produ...  04 October, 2020        \n",
       "0  Tasmanian devils have been reintroduced into t...  05 October, 2020        \n",
       "0  Happy anniversary, PBS: today marks 50 years o...  04 October, 2020        \n",
       "\n",
       "                    root_url  \n",
       "0  https://www.autocar.co.uk  \n",
       "0        https://www.bbc.com  \n",
       "0    https://www.nytimes.com  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scraped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
